{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld with zero reward for each step\n",
    "In this Jupyter Notebook I will rebuild and analyze the gridworld example from https://github.com/dennybritz/reinforcement-learning assigning zero reward instead of -1 for each step, and assignin a positive reward when ending on one of the two terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I import my custom gridworld environment that I have defined in gridworld_zero_reward.py\n",
    "import numpy as np\n",
    "from gridworld_zero_reward import GridworldEnv_zero_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generate the 4x4 gridworld enviroment with zero reward for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_4 = GridworldEnv_zero_reward([4,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I define the policy evaluation and improvement functions to execute the policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with a random (all 0) value function\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        diff = 0\n",
    "        for s in range(env.nS):\n",
    "            vn=0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, prox_stato, ricompensa, term in env.P[s][a]:\n",
    "                    vn += action_prob * prob * (ricompensa + discount_factor * V[prox_stato])\n",
    "            diff = max(diff, np.abs(vn - V[s]))\n",
    "            V[s] = vn       \n",
    "        if diff < theta:\n",
    "            break\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the policy improvement algorithm I had to insert a check to verify whether the value for each action in a state is equal, in that case the updated policy choose the same action as the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm (improvement sarebbe il greedy). Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "    def one_step_lookahead(state, V):\n",
    "        \"\"\"\n",
    "        Helper function to calculate the value for all action in a given state.\n",
    "        \n",
    "        Args:\n",
    "            state: The state to consider (int)\n",
    "            V: The value to use as an estimator, Vector of length env.nS\n",
    "        \n",
    "        Returns:\n",
    "            A vector of length env.nA containing the expected value of each action.\n",
    "        \"\"\"\n",
    "        A = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * V[next_state])\n",
    "        return A\n",
    "    \n",
    "    policy_stable = True\n",
    "    \n",
    "    while True:\n",
    "        V = policy_eval_fn(policy, env, discount_factor)\n",
    "        policy_stable = True\n",
    "        for s in range(env.nS):\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            action_values = one_step_lookahead(s, V) \n",
    "            best_a = np.argmax(action_values) \n",
    "            # Now I check if the action values are all the same, in this case I keep the same action as before \n",
    "            if np.unique(action_values).size == 1:\n",
    "                best_a = chosen_a\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.nA)[best_a]\n",
    "        if policy_stable:\n",
    "            return policy, V\n",
    "    return policy, np.zeros(env.nS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i test the policy iteration with discount factor = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_09, v_09 = policy_improvement(env_4, discount_factor=0.9)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy_09)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I display the value function and the policy on the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Value Function:\n",
      "[[  0. 100.  90.  81.]\n",
      " [100.  90.  81.  90.]\n",
      " [ 90.  81.  90. 100.]\n",
      " [ 81.  90. 100.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v_09.reshape(env_4.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy:\n",
      "[[↑ ← ← ↓]\n",
      " [↑ ↑ ↓ ↓]\n",
      " [↑ → → ↓]\n",
      " [↑ → → ↑]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy:\")\n",
    "policy_grid=str(np.reshape(np.argmax(policy_09, axis=1), env_4.shape))\n",
    "policy_grid=policy_grid.replace('0', '↑')\n",
    "policy_grid=policy_grid.replace('1', '→')\n",
    "policy_grid=policy_grid.replace('2', '↓')\n",
    "policy_grid=policy_grid.replace('3', '←')\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now i test the policy improvement method imposing the discount factor equal to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_4=GridworldEnv_zero_reward([4,4])\n",
    "policy_1, v_1 = policy_improvement(env_4, discount_factor=1)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy_1)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Value Function:\n",
      "[[  0. 100. 100. 100.]\n",
      " [100. 100. 100. 100.]\n",
      " [100. 100. 100. 100.]\n",
      " [100. 100. 100.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v_1.reshape(env_4.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy:\n",
      "[[↑ ← ← ↓]\n",
      " [↑ ↑ ↓ ↓]\n",
      " [↑ → → ↓]\n",
      " [→ → → ↑]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy:\")\n",
    "policy_grid=str(np.reshape(np.argmax(policy_1, axis=1), env_4.shape))\n",
    "policy_grid=policy_grid.replace('0', '↑')\n",
    "policy_grid=policy_grid.replace('1', '→')\n",
    "policy_grid=policy_grid.replace('2', '↓')\n",
    "policy_grid=policy_grid.replace('3', '←')\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly different but the policy is optimal in both cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I consider a larger 10x10 gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_10 = GridworldEnv_zero_reward([10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_10, v_10 = policy_improvement(env_10, discount_factor=1)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy_10)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy:\n",
      "[[↑ ← ← ← ← ← ← ← ← ↓]\n",
      " [↑ ↑ ← ← ← ← ← ← ↓ ↓]\n",
      " [↑ ↑ ↑ ← ← ← ← ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ ↑ ← ← ↓ ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ ↑ → → ↓ ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ → → → → ↓ ↓ ↓]\n",
      " [↑ ↑ → → → → → → ↓ ↓]\n",
      " [↑ → → → → → → → → ↓]\n",
      " [→ → → → → → → → → ↑]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy:\")\n",
    "policy_grid=str(np.reshape(np.argmax(policy_10, axis=1), env_10.shape))\n",
    "policy_grid=policy_grid.replace('0', '↑')\n",
    "policy_grid=policy_grid.replace('1', '→')\n",
    "policy_grid=policy_grid.replace('2', '↓')\n",
    "policy_grid=policy_grid.replace('3', '←')\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Value Function:\n",
      "[[  0. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100. 100.]\n",
      " [100. 100. 100. 100. 100. 100. 100. 100. 100.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v_10.reshape(env_10.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to try a rectangular shaped gridowrld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_16x9 = GridworldEnv_zero_reward([16,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_16x9, v_16x9 = policy_improvement(env_16x9, discount_factor=1)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy_16x9)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy:\n",
      "[[↑ ← ← ← ← ← ← ← ←]\n",
      " [↑ ← ← ← ← ← ← ← ←]\n",
      " [↑ ↑ ← ← ← ← ← ← ←]\n",
      " [↑ ↑ ↑ ← ← ← ← ← ←]\n",
      " [↑ ↑ ↑ ↑ ← ← ← ← ←]\n",
      " [↑ ↑ ↑ ↑ ↑ ← ← ← ↓]\n",
      " [↑ ↑ ↑ ↑ ↑ ← ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓]\n",
      " [↑ ↑ ↑ → ↓ ↓ ↓ ↓ ↓]\n",
      " [↑ → → → ↓ ↓ ↓ ↓ ↓]\n",
      " [→ → → → ↓ ↓ ↓ ↓ ↓]\n",
      " [→ → → → → ↓ ↓ ↓ ↓]\n",
      " [→ → → → → → ↓ ↓ ↓]\n",
      " [→ → → → → → → ↓ ↓]\n",
      " [→ → → → → → → → ↓]\n",
      " [→ → → → → → → → ↑]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy:\")\n",
    "policy_grid=str(np.reshape(np.argmax(policy_16x9, axis=1), env_16x9.shape))\n",
    "policy_grid=policy_grid.replace('0', '↑')\n",
    "policy_grid=policy_grid.replace('1', '→')\n",
    "policy_grid=policy_grid.replace('2', '↓')\n",
    "policy_grid=policy_grid.replace('3', '←')\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, lets try with a very large gridworld (30x30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_30x30 = GridworldEnv_zero_reward([30,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_30x30, v_30x30= policy_improvement(env_30x30, discount_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy:\n",
      "[↑ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↑ ↑ ← ← ← ← ←\n",
      " ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ←\n",
      " ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ←\n",
      " ← ← ← ← ← ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓\n",
      " ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑\n",
      " ↑ ↑ ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ←\n",
      " ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ←\n",
      " ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓\n",
      " ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑\n",
      " ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑\n",
      " ↑ ↑ ↑ ← ← ← ← ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ← ← ↓\n",
      " ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\n",
      " ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑\n",
      " ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑\n",
      " ↑ ↑ ↑ ↑ → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → → →\n",
      " → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → → → → → → → → → ↓ ↓\n",
      " ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → → → → → → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\n",
      " ↓ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ → → → → → → → → → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑\n",
      " ↑ → → → → → → → → → → → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ ↑ → → → → → → →\n",
      " → → → → → → → → → → → ↓ ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ ↑ → → → → → → → → → → → → → → →\n",
      " → → → → → ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↑ → → → → → → → → → → → → → → → → → → → → → → ↓\n",
      " ↓ ↓ ↓ ↑ ↑ ↑ → → → → → → → → → → → → → → → → → → → → → → → → ↓ ↓ ↓ ↑ ↑ → →\n",
      " → → → → → → → → → → → → → → → → → → → → → → → → ↓ ↓ ↑ → → → → → → → → → →\n",
      " → → → → → → → → → → → → → → → → → → ↓ → → → → → → → → → → → → → → → → → →\n",
      " → → → → → → → → → → → ↑]\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy:\")\n",
    "policy_grid=str(np.reshape(np.argmax(policy_30x30, axis=1), v_30x30.shape))\n",
    "policy_grid=policy_grid.replace('0', '↑')\n",
    "policy_grid=policy_grid.replace('1', '→')\n",
    "policy_grid=policy_grid.replace('2', '↓')\n",
    "policy_grid=policy_grid.replace('3', '←')\n",
    "print(policy_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the examples shown above,the policy returned for this modified version of the traditional gridworld is still the best in every case. This behavior is expected if we set the discount factor less than 1, because the factor reduces the value of the future states. If instead the discount factor is set to 1 this behaviour is quite unexpected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
